---
title: "Machine Learning with `tidymodels`: Part 2" 
theme: readable
output:
  html_document:
    toc: true
    toc_float: true
    fig_width: 12
    fig_height: 7
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings({ 
  library(tidymodels)
  library(here)
  library(palmerpenguins)
  library(tidyverse)
  library(doParallel)
}))
# prefer tidymodels functions in any case of name conflict
tidymodels::tidymodels_prefer() 
```

### Machine Learning Analysis: A Template 

Most machine learning analyses follow a similar set of steps. Our discussion of `tidymodels` will be built around this structure. In any analysis, we expect to do the following, often over multiple iterations: 

!["The Modeling Process. Taken from Kuhn and Silge 2021](https://www.tmwr.org/premade/modeling-process.svg)
The figure shows the primary modeling steps. Analysts:
  - Build a training and test set as well as conduct exploratory data analysis
  - Decide if and how to pre-process data to be appropriate for modeling,
  - build and fit models on a training set
  - Evaluate models according to selected metrics 
  - Refine their models
  - Evaluate their chosen model against the test set

Let's take each of these steps in terms and show how to implement them with `tidymodels`. To introduce features in `tidymodels` we will focus on a single dataset and a single algorithm--linear regression. We will see in the third part of the workshop that the same structure of the `tidymodels` code applies to a host of different modeling approaches. That's the main benefit!

### Building a training and test set 

When modeling, a common technique to best use the data available for prediction is to split the data into groups. The most common split is two groups, one referred to as the "training" data and one referred to as the "test" data. The training data is used to develop models and feature sets, and comparing between different algorithms. The test set is only used at the conclusion of all model development to estimate a final, unbiased assessment of the model's performance. 

In resources on the `tidymodels` framework, this process is often referred to as the *data spend*. The functions necessary for splitting the data are located in the `rsample` package that is loaded with `tidymodels`. To make a train/test split, we use `initial_split()` from the `rsample` package. 
 
```{r}
## package here is rsample 
penguins <- penguins
set.seed(12345)
pen_split <- penguins %>% 
  initial_split(prop = .8)
```

This type of split presumes that all units in our data are independent. We might be concerned that such a random sample would not be the optimal training set. In this case, we can *stratify* our data set according to a variable of interest. In general, stratifying (also known as blocking) is never a bad strategy and can increase precision (Gerber and Green 2012).  

```{r}
# Stratify by species to ensure equal representation
penguins_split_strata <- penguins %>% 
  initial_split(prop = 0.80, strata = species)
```

The resulting object is an `rsplit` object which contains the partitioning information for the data. To get the training and test data we apply two additional function. The resulting datasets have the same columns as the original data, but only the appropriately sampled rows. 

```{r}
penguins_train <- training(penguins_split_strata)
penguins_test  <-  testing(penguins_split_strata)
```

### Workflow process 

#### Data preprocessing with `recipes` 

Sometimes the features we have are not in the best format for our model to use effectively. Feature Engineering refers to all the activities necessary to reformat features to improve the modeling process. Within the `tidymodels` framework, the functions to implement feature engineering are housed in the `recipes` package. 

A recipe is an object that defines the series of steps needed for any data processing for the model. Importantly, recipes define the steps to be taken without immediately executing them. This 

A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done. 

Here is an example recipe for a machine learning model to predict bill length from other features. 
```{r}
penguins_recipe <- recipe(bill_length_mm ~ ., data = penguins_train)%>%
  step_dummy(all_nominal_predictors())
```

In this recipe, we provide the model that we eventually want to run and a step to convert nominal features into dummy variables. The call to `recipe()` with a formula tells R the roles of the variables. Here our dependent variable is `bill_length_mm`. In addition, the data argument tells `tidymodels` to only fit values in the training set. step_dummy() is used to specify which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables. 

The function all_nominal_predictors() captures the names of any predictor columns that are currently factor or character. `tidymodels` has lots of data preprocessing step functions [available](https://recipes.tidymodels.org/reference/index.html). While we will not cover how to do so, it is also possible to write custom preprocessing functions. 

### Challenge 4 

Sometimes features on different scales can lead to strange predictions and poor test fit. Therefore, one step that analysts may take is to center and normalize their data. Using the reference link, identify the function that normalizes data and add it to the recipe to apply it all numeric predictors in the data. Call your new recipe "challenge4_recipe"

```{r}

```


#### Model fitting with `parsnip` 

Within the `tidymodels` package, the `parsnip` package provides a fluent and standardized interface for a variety of models (Kuhn and Silge 2021). This approach to modeling follows the design paradigm of the package. For a user, we first specify the *type* of model we want to fit. 

The advantage of `tidymodels` is that model arguments are the same relative to the underlying engine. For example, when we learn about random forests, we will see that three commonly used arguments are the number of trees to create in an ensemble, the number of features to randomly sample at each split, and the number of units needed for a split. Three popular packages for implementing random forests in R use different arguments for each of these. In `tidymodels` you only have to remember one set of arguments, cutting down on cognitive overload. 

We then specify the *engine* to fit the model. The engine is the underlying software to fit the model. 

Recall that we want to fit a linear regression. In R, there are multiple packages that fit a linear regression. Here is the code for three of them in `tidymodels`

```{r}
# default R 
linear_reg()%>%
  set_engine("lm")

# glmnet 
linear_reg()%>%
  set_engine("glmnet")

# Using stan 
linear_reg %>% 
  set_engine("stan")
```

Finally, if required we declare the *mode* of the model. This means that we specify if this is a regression problem or a classification problem. In our case, since we are interested in predicting bill length, we need to set the mode to regression. The function for that is `set_mode()`.

Here is a full initialization of our model. To demonstrate the fitting, we hold off for now on combining the model fit with a recipe. We will do that in the next section on workflows. 

```{r}
lm_model <- linear_reg()%>%
  set_engine("lm")%>%
  set_mode("regression")%>%
  # fit a model by hand as an example 
  fit(bill_length_mm ~ . , data = penguins_train)

# We can see the model that we created with the summary() method 
lm_model %>% 
  pluck('fit')%>%
  summary()%>%
  # and we can tidy the output with tidy()
  tidy()

```

### Challenge 5 

Write and fit a logistic regression specification to classify penguin species based on all other features. You will have to make three changes from the linear specification. Use `glm` as your engine. Since this is for practice, do not worry about any warning messages that may appear. 

```{r}

```

Once we fit a model, we can use our model to predict new points. In `tidymodels` predictions always conform to the following rules. 

1. The results are always a tibble.

2. The column names of the tibble are always predictable. The table below shows how `tidymodels` labels values. 

|type value |column name(s)|
|-----------|--------------|
|numeric 	|.pred|
|class 	|.pred_class|
|prob 	|.pred_{class levels}|
|conf_int |	.pred_lower, .pred_upper|
|pred_int |	.pred_lower, .pred_upper|


3. There are always as many rows in the tibble as in the input data set.

To demonstrate prediction, we will take a small sample of the test set.

```{r}
prediction_demo <- penguins_test %>% 
  slice(1:5)

prediction_demo %>% 
  predict(lm_model, new_data = .)
```

#### Model workflows with `workflow`

For small tests and quick projects, we can fit models directly as shown in the last section. To transition to workflows, let's first see how we can apply our recipe directly to data, and then show how the same process is done in a workflow. 

A problem with our previous regression is that we did not do any of the preprocessing that we specified in our recipe. For that matter, we did not use the recipe at all. To do that directly, we need to apply two functions: `prep()` and `bake()`. We `prep()` any recipe that has at least one preprocessing operation. `prep()` returns our updated recipe with the appropriate estimates. Think of prep in the baking analogy to getting all your ingredients ready to go on the counter. After we `prep()` the recipe, we have to `bake()` the recipe by taking the preprocessing steps and applying them to a data set. 

Here's an example that fits our linear regression model using the recipe from Challenge 4. 

```{r}
challenge4_recipe <- recipe(bill_length_mm ~ ., data = penguins_train)%>%
  step_normalize(all_numeric_predictors())

linear_reg()%>%
  set_engine("lm")%>%
  set_mode("regression")%>%
  # fit a model by hand as an example 
  fit(bill_length_mm ~ . , data = bake(prep(challenge4_recipe),penguins_train))%>%
  tidy()
```

While that certainly works, the code looks clunky and can be difficult to organize. Furthermore, if we want to change aspects of this call we have to make several changes. 

Enter the `workflows` package. A workflow is the set of steps that `tidymodels` should run to execute an analysis. A workflow is initialized by the function `workflow()` and always requires a parsnip object. Here is the same result as the previous computation done in workflow form. 

```{r}
lm_model <- 
  linear_reg() %>% 
  set_engine("lm")

penguins_lm_wflow <- workflow() %>% 
  add_recipe(challenge4_recipe)%>%
  add_model(lm_model)

penguins_lm_fit <- fit(penguins_lm_wflow, penguins_train)

# tidy our model fit for readability 
penguins_lm_fit %>% 
  tidy()

# predict new observations of our model fit with predict()
# An additional advantage of the workflow is that we do not have to 
# duplicate the preprocessing steps on our test set.
predict(penguins_lm_fit, new_data = penguins_test %>% slice(1:5))
```

### Challenge 6 

Write a new recipe that predicts bill length from bill depth, flipper length, and body mass only. Normalize all variables. Write a workflow that uses your new recipe to fit a linear model. 



```{r}

```


#### Model evaluation with `yardstick` 

Now we have a model. It's a basic model, and the next reasonable question to ask is how well does it work to solve the problem of estimating the relationship. We prefer to have a quantiative approach to estimate effectiveness to be able to compare different models or to tweak our model to improve performance. In `tidymodels` this approach is empirically data-driven (Kuhn and Silge 2021). That means that we use the test data to measure the model's effectiveness. 

It is important here to note once again that we keep the training and test dataset apart. We can run any tweaks that we want to our training set, but we should leave the test set alone until we are ready to evaluate our models. Methods for evaluation within the the `tidymodels` universe are from the `yardstick` package. 

The general syntax for a metrics function in `tidymodels` is 

```{r, eval = F}
function(data, truth, ...)

```

where the data argument is a data frame or tibble and the truth argument is the column with observed outcome values. Additional arguments (...) can be used to specify columns containing features. 

As we have seen, the `predict()` method returns a tibble with the predictions from our model on new data. We can match these values with the corresponding observed outcome values. 

```{r}
penguins_test_results <- predict(penguins_lm_fit, new_data = penguins_test)%>%
  bind_cols(penguins_test %>% select(bill_length_mm))

# We can plot the data prior to computing metrics for a visual inspection of fit
penguins_test_results %>%
  ggplot(aes(x = bill_length_mm, y = .pred))+
  geom_abline(lty = 2)+
  geom_point(alpha = 0.7)+
  labs(x = "Bill Length (mm)", y = "Predicted Values")+
  coord_obs_pred()
```

The full suite of metrics functions are available [here](https://yardstick.tidymodels.org/reference/index.html). Since our running example is a linear regression, let's pick three metrics that are common to linear regression models: Root Mean Square Error (RMSE), $R^2$, and Mean Absolute Error (MAE). 

```{r}
# Make a set of metrics
penguins_metrics <- metric_set(rmse, rsq, mae)
penguins_metrics(penguins_test_results, truth = bill_length_mm, estimate = .pred)
```

When comparing between models, we tend to prefer models with lower error rates and higher explanatory power. 

#### Resampling with `rsample` 

Resampling methods are a way of simulating the data generating process using some empirical data for modeling and different data for evaluation. The process of resampling is repeated iteratively. The test set is held out for the same reasons as before. For each iteration of resampling, the training data is partitioned into a set for analysis and a set for assessment. In `tidymodels` the tools for resampling are based in the `rsample` package. 

Cross Validation is the most common resampling approach. In cross validation, we split the iterations into *folds* and follow the iterative process. In the extreme, we could have *n* folds where n is the number of training data points. This would occur in Leave-One-Out-Cross-Validation where the subsets have one point for the assessment set and the remainder in the analysis set. 

LOOCV is a special case of *k-fold* cross validation, In this approach we randomly divide the set of training data into k groups of approximately equal sizes. We then iterate over each fold, using it as the analysis set and the remaining data as the assessment data. There is a bias-variance trade-off associated with the choice of k, such that LOOCV tends to have higher variance on test error than more limited folds (James *et al* 2021). The typical rule of thumb is to perform cross validation with k between 5 and 10, as these values empirically yield test error rates that do not have excessively high bias or high variance (James *et al* 2021).

We implement cross validation in `tidymodels` like so. 

```{r}
set.seed(1234)
# tidymodels uses v where much of the theoretical literature uses k
# we can also set the replicates of the folds with the repeats argument
penguins_folds <- vfold_cv(penguins_train, v = 10)
penguins_folds

## Perform resampling fits
penguins_cv_res <- penguins_lm_wflow %>%
  fit_resamples(resamples = penguins_folds, control = control_resamples(save_pred = TRUE))

## see metrics 
penguins_cv_res %>%
  collect_metrics()


## Get the assessment set predictions 
penguins_cv_res %>% 
  collect_predictions()
```

Our comparison reveals that we can fit better models via cross validation than with just a single training set. This is reflected by the lower RMSE and higher $R^2$ averages. 

From a computation perspective, each of these fits are "embarrasingly parallel" in the sense that all of them could be fit simultaneously without issues. Speed gains can occur by splitting computations across processors on the same computer, or across different computers in a cloud environment. Use the `parallel` package to take advantage of speed gains if that becomes an issue for your data. 


For computations conducted on a single computer, the number of possible “worker processes” is determined by the `doParallel` package:

```{r}
# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)

penguins_cv_res <- penguins_lm_wflow %>%
  fit_resamples(resamples = penguins_folds, control = control_resamples(save_pred = TRUE))

stopCluster(cl)

## see metrics 
penguins_cv_res %>%
  collect_metrics()

## Get the assessment set predictions 
penguins_cv_res %>% 
  collect_predictions()


### Run this function afterwards to confirm that computing connections close correctly 
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
unregister_dopar()
```

#### Model optimization with `tune`

Tuning parameters or hyperparameters are often found in machine learning models:

Tuning parameter optimization usually falls into one of two categories:
  - pre-define a set of parameter values to evaluate or
  - sequentially discover new parameter combinations based on previous results.


How can we signal to tidymodels functions which arguments should be optimized? Parameters are marked for tuning by assigning them a value of tune(). Let's demonstrate by considering whether our model would be improved by creating polynomials for a parameter. We will try this with the body mass variable. 

Since tuning changes the model type, this requires we add a preprocessing step, so we build a new recipe first. Following that we can build a workflow, and then use cross-validation to tune our parameters.

```{r}
## First update our recipe 
penguins_poly_recipe <- challenge4_recipe %>%
  # Step to omit nas. Set skip to true to avoid
  # skipping at assessment phase unnecessarily
  step_naomit(body_mass_g, skip = TRUE)%>%
  step_poly(body_mass_g, degree = tune())

tuned_wflow <- workflow()%>%
  add_recipe(penguins_poly_recipe)%>%
  add_model(lm_model)

cv_splits <- vfold_cv(penguins_train, v = 10)
degree_grid <- grid_regular(degree(range =c(1,5)), levels = 5)
## resampling specification 
tune_res <- tune_grid(
  object = tuned_wflow,
  resamples = cv_splits,
  grid = degree_grid
)

# We can get the average metric value for each parameter combination 
tune_res %>% 
  collect_metrics()

## We can show the best model 
tune_res %>%
  show_best(metric = "rmse")
```

The results here show that the best performing model is a polynomial of degree 2, though it's functionally identical to a model with no polynomial. Here, that's in part because we have no reason to believe that adding a polynomial on body mass should greatly improve our ability to predict bill length. We will see in the next section how tuning can matter a lot for model accuracy with different data and algorithms. 
