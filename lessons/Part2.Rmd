---
title: "Machine Learning with `tidymodels`: Part 2" 
theme: readable
output:
  html_document:
    toc: true
    toc_float: true
    fig_width: 12
    fig_height: 7
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings({ 
  library(tidymodels)
  library(here)
  library(palmerpenguins)
  library(tidyverse)
}))
# prefer tidymodels functions in any case of name conflict
tidymodels::tidymodels_prefer() 
```

### Machine Learning Analysis: A Template 

Most machine learning analyses follow a similar set of steps. Our discussion of `tidymodels` will be built around this structure. In any analysis, we expect to do the following, often over multiple iterations: 

!["The Modeling Process. Taken from Kuhn and Silge 2021](https://www.tmwr.org/premade/modeling-process.svg)
The figure shows the primary modeling steps. Analysts:
  - Build a training and test set as well as conduct exploratory data analysis
  - Decide if and how to pre-process data to be appropriate for modeling,
  - build and fit models on a training set
  - Evaluate models according to selected metrics 
  - Refine their models
  - Evaluate their chosen model against the test set

Let's take each of these steps in terms and show how to implement them with `tidymodels`. To introduce features in `tidymodels` we will focus on a single dataset and a single algorithm--linear regression. We will see in the third part of the workshop that the same structure of the `tidymodels` code applies to a host of different modeling approaches. That's the main benefit!

### Building a training and test set 

When modeling, a common technique to best use the data available for prediction is to split the data into groups. The most common split is two groups, one referred to as the "training" data and one referred to as the "test" data. The training data is used to develop models and feature sets, and comparing between different algorithms. The test set is only used at the conclusion of all model development to estimate a final, unbiased assessment of the model's performance. 

In resources on the `tidymodels` framework, this process is often referred to as the *data spend*. The functions necessary for splitting the data are located in the `rsample` package that is loaded with `tidymodels`. To make a train/test split, we use `initial_split()` from the `rsample` package. 
 
```{r}
## package here is rsample 
penguins <- penguins
set.seed(12345)
pen_split <- penguins %>% 
  initial_split(prop = .8)
```

This type of split presumes that all units in our data are independent. We might be concerned that such a random sample would not be the optimal training set. In this case, we can *stratify* our data set according to a variable of interest. In general, stratifying (also known as blocking) is never a bad strategy and can increase precision (Gerber and Green 2012).  

```{r}
# Stratify by species to ensure equal representation
penguins_split_strata <- penguins %>% 
  initial_split(prop = 0.80, strata = species)
```

The resulting object is an `rsplit` object which contains the partitioning information for the data. To get the training and test data we apply two additional function. The resulting datasets have the same columns as the original data, but only the appropriately sampled rows. 

```{r}
penguins_train <- training(penguins_split_strata)
penguins_test  <-  testing(penguins_split_strata)
```

### Workflow process 

#### Data preprocessing with `recipes` 

Sometimes the features we have are not in the best format for our model to use effectively. Feature Engineering refers to all the activities necessary to reformat features to improve the modeling process. Within the `tidymodels` framework, the functions to implement feature engineering are housed in the `recipes` package. 

A recipe is an object that defines the series of steps needed for any data processing for the model. Importantly, recipes define the steps to be taken without immediately executing them. This 

A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done. 

Here is an example recipe for a machine learning model to predict bill length from other features. 
```{r}
penguins_recipe <- recipe(bill_length_mm ~ ., data = penguins_train)%>%
  step_dummy(all_nominal_predictors())
```

In this recipe, we provide the model that we eventually want to run and a step to convert nominal features into dummy variables. The call to `recipe()` with a formula tells R the roles of the variables. Here our dependent variable is `bill_length_mm`. In addition, the data argument tells `tidymodels` to only fit values in the training set. step_dummy() is used to specify which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables. 

The function all_nominal_predictors() captures the names of any predictor columns that are currently factor or character. `tidymodels` has lots of data preprocessing step functions [available](https://recipes.tidymodels.org/reference/index.html). While we will not cover how to do so, it is also possible to write custom preprocessing functions. 

### Challenge 4 

Sometimes features on different scales can lead to strange predictions and poor test fit. Therefore, one step that analysts may take is to center and normalize their data. Using the reference link, identify the function that normalizes data and add it to the recipe to apply it all numeric predictors in the data. Call your new recipe "challenge4_recipe"

```{r}

```


#### Model fitting with `parsnip` 

Within the `tidymodels` package, the `parsnip` package provides a fluent and standardized interface for a variety of models (Kuhn and Silge 2021). This approach to modeling follows the design paradigm of the package. For a user, we first specify the *type* of model we want to fit. 

The advantage of `tidymodels` is that model arguments are the same relative to the underlying engine. For example, when we learn about random forests, we will see that three commonly used arguments are the number of trees to create in an ensemble, the number of features to randomly sample at each split, and the number of units needed for a split. Three popular packages for implementing random forests in R use different arguments for each of these. In `tidymodels` you only have to remember one set of arguments, cutting down on cognitive overload. 

We then specify the *engine* to fit the model. The engine is the underlying software to fit the model. 

Recall that we want to fit a linear regression. In R, there are multiple packages that fit a linear regression. Here is the code for three of them in `tidymodels`

```{r}
# default R 
linear_reg()%>%
  set_engine("lm")

# glmnet 
linear_reg()%>%
  set_engine("glmnet")

# Using stan 
linear_reg %>% 
  set_engine("stan")
```

Finally, if required we declare the *mode* of the model. This means that we specify if this is a regression problem or a classification problem. In our case, since we are interested in predicting bill length, we need to set the mode to regression. The function for that is `set_mode()`.

Here is a full initialization of our model. To demonstrate the fitting, we hold off for now on combining the model fit with a recipe. We will do that in the next section on workflows. 

```{r}
lm_model <- linear_reg()%>%
  set_engine("lm")%>%
  set_mode("regression")%>%
  # fit a model by hand as an example 
  fit(bill_length_mm ~ . , data = penguins_train)

# We can see the model that we created with the summary() method 
lm_model %>% 
  pluck('fit')%>%
  summary()%>%
  # and we can tidy the output with tidy()
  tidy()

```

### Challenge 5 

Write and fit a logistic regression specification to classify penguin species based on all other features. You will have to make three changes from the linear specification. Use `glm` as your engine. Since this is for practice, do not worry about any warning messages that may appear. 

```{r}

```

Once we fit a model, we can use our model to predict new points. In `tidymodels` predictions always conform to the following rules. 

1. The results are always a tibble.

2. The column names of the tibble are always predictable. The table below shows how `tidymodels` labels values. 

|type value |column name(s)|
|-----------|--------------|
|numeric 	|.pred|
|class 	|.pred_class|
|prob 	|.pred_{class levels}|
|conf_int |	.pred_lower, .pred_upper|
|pred_int |	.pred_lower, .pred_upper|


3. There are always as many rows in the tibble as in the input data set.

To demonstrate prediction, we will take a small sample of the test set.

```{r}
prediction_demo <- penguins_test %>% 
  slice(1:5)

prediction_demo %>% 
  predict(lm_model, new_data = .)
```

#### Model workflows with `workflow`

For small tests and quick projects, we can fit models directly as shown in the last section. To transition to workflows, let's first see how we can apply our recipe directly to data, and then show how the same process is done in a workflow. 

A problem with our previous regression is that we did not do any of the preprocessing that we specified in our recipe. For that matter, we did not use the recipe at all. To do that directly, we need to apply two functions: `prep()` and `bake()`. We `prep()` any recipe that has at least one preprocessing operation. `prep()` returns our updated recipe with the appropriate estimates. Think of prep in the baking analogy to getting all your ingredients ready to go on the counter. After we `prep()` the recipe, we have to `bake()` the recipe by taking the preprocessing steps and applying them to a data set. 

Here's an example that fits our linear regression model using the recipe from Challenge 4. 

```{r}
challenge4_recipe <- recipe(bill_length_mm ~ ., data = penguins_train)%>%
  step_normalize(all_numeric_predictors())

linear_reg()%>%
  set_engine("lm")%>%
  set_mode("regression")%>%
  # fit a model by hand as an example 
  fit(bill_length_mm ~ . , data = bake(prep(challenge4_recipe),penguins_train))%>%
  tidy()
```

While that certainly works, the code looks clunky and can be difficult to organize. Furthermore, if we want to change aspects of this call we have to make several changes. 

Enter the `workflows` package. A workflow is the set of steps that `tidymodels` should run to execute an analysis. A workflow is initialized by the function `workflow()` and always requires a parsnip object. Here is the same result as the previous computation done in workflow form. 

```{r}
lm_model <- 
  linear_reg() %>% 
  set_engine("lm")

penguins_lm_wflow <- workflow() %>% 
  add_recipe(challenge4_recipe)%>%
  add_model(lm_model)

penguins_lm_fit <- fit(penguins_lm_wflow, penguins_train)

# tidy our model fit for readability 
penguins_lm_fit %>% 
  tidy()

# predict new observations of our model fit with predict()
# An additional advantage of the workflow is that we do not have to 
# duplicate the preprocessing steps on our test set.
predict(penguins_lm_fit, new_data = penguins_test %>% slice(1:5))
```

### Challenge 6 

Write a new recipe that predicts bill length from bill depth, flipper length, and body mass only. Normalize all variables. Write a workflow that uses your new recipe to fit a linear model. 



```{r}

```


#### Model evaluation with `yardstick` 

Now we have a model. It's a basic model, and the next reasonable question to ask is how well does it work to solve the problem of estimating the relationship. We prefer to have a quantiative approach to estimate effectiveness to be able to compare different models or to tweak our model to improve performance. In `tidymodels` this approach is empirically data-driven (Kuhn and Silge 2021). That means that we use the test data to measure the model's effectiveness. 

It is important here to note once again that we keep the training and test dataset apart. We can run any tweaks that we want to our training set, but we should leave the test set alone until we are ready to evaluate our models. Methods for evaluation within the the `tidymodels` universe are from the `yardstick` package. 

The general syntax for a metrics function in `tidymodels` is 

```{r, eval = F}
function(data, truth, ...)

```

where the data argument is a data frame or tibble and the truth argument is the column with observed outcome values. Additional arguments (...) can be used to specify columns containing features. 

As we have seen, the `predict()` method returns a tibble with the predictions from our model on new data. We can match these values with the corresponding observed outcome values. 

```{r}
penguins_test_results <- predict(penguins_lm_fit, new_data = penguins_test)%>%
  bind_cols(penguins_test %>% select(bill_length_mm))

# We can plot the data prior to computing metrics for a visual inspection of fit
penguins_test_results %>%
  ggplot(aes(x = bill_length_mm, y = .pred))+
  geom_abline(lty = 2)+
  geom_point(alpha = 0.7)+
  labs(x = "Bill Length (mm)", y = "Predicted Values")+
  coord_obs_pred()
```

The full suite of metrics functions are available [here](https://yardstick.tidymodels.org/reference/index.html). Since our running example is a linear regression, let's pick three metrics that are common to linear regression models: Root Mean Square Error (RMSE), $R^2$, and Mean Absolute Error (MAE). 

```{r}
# Make a set of metrics
penguins_metrics <- metric_set(rmse, rsq, mae)
penguins_metrics(penguins_test_results, truth = bill_length_mm, estimate = .pred)
```

#### Resampling with `rsample` 

Cross Validation 

The column named splits contains the information on how to split the data (similar to the object used to create the initial training/test partition).

Packages for this are `rsample` and estimating performance with `tune`

```{r}
set.seed(55)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds

## Manually retrieve partitioned data 
# For the first fold:
ames_folds$splits[[1]] %>% 
  analysis() 

```

To create a validation set object that uses 3/4 of the data for model fitting:
```{r}
set.seed(12)
val_set <- validation_split(ames_train, prop = 3/4)
val_set
```

Bootstrapping methods using rsample:

```{r}
bootstraps(ames_train, times = 5)

```


Any of these resampling methods can be used to evaluate the modeling process (including preprocessing, model fitting, etc). These methods are effective because different groups of data are used to train the model and assess the model. To reiterate the process:

1. During resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model.

2. The preprocessing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance.

This sequence repeats for every resample. If there are B resamples, there are B replicates of each of the performance metrics. The final resampling estimate is the average of these B statistics. If B = 1, as with a validation set, the individual statistics represent overall performance.

Parallel Processing 

The models created during resampling are independent of one another. Computations of this kind are sometimes called “embarrassingly parallel”; each model could be fit simultaneously without issues. The tune package uses the foreach package to facilitate parallel computations. These computations could be split across processors on the same computer or across different computers, depending on the chosen technology.

For computations conducted on a single computer, the number of possible “worker processes” is determined by the parallel package:

```{r}
# The number of physical cores in the hardware:
parallel::detectCores(logical = FALSE)


# The number of possible independent processes that can 
# be simultaneously used:  
parallel::detectCores(logical = TRUE)

# All operating systems
library(doParallel)

# Create a cluster object and then register: 
cl <- makePSOCKcluster(2)
registerDoParallel(cl)

# Now run fit_resamples()`...

stopCluster(cl)

```

General Code through Resampling 

```{r}
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(123)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(55)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(130)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)

```

#### Model optimization with `tune`

Tuning parameters or hyperparameters are often found in machine learning models:

Boosting is an ensemble method that combines a series of base models, each of which is created sequentially and depends on the previous models. The number of boosting iterations is an important parameter that usually requires optimization.

In the classic single-layer artificial neural network (a.k.a. the multilayer perceptron), the predictors are combined using two or more hidden units. The hidden units are linear combinations of the predictors that are captured in an activation function (typically a nonlinear function, such as a sigmoid). The hidden units are then connected to the outcome units; one outcome unit is used for regression models and multiple outcome units are required for classification. The number of hidden units and the type of activation function are important structural tuning parameters.

Modern gradient descent methods are improved by finding the right optimization parameters. Examples are learning rates, momentum, and the number of optimization iterations/epochs (Goodfellow, Bengio, and Courville 2016). Neural networks and some ensemble models use gradient descent to estimate the model parameters. While the tuning parameters associated with gradient descent are not structural parameters, they often require tuning.

In some cases, preprocessing techniques require tuning:

In principal component analysis, or its supervised cousin called partial least squares, the predictors are replaced with new, artificial features that have better properties related to collinearity. The number of extracted components can be tuned.

Imputation methods estimate missing predictor values using the complete values of one or more predictors. One effective imputation tool uses K-nearest neighbors of the complete columns to predict the missing value. The number of neighbors modulates the amount of averaging and can be tuned.

Some classical statistical models also have structural parameters:

In binary regression, the logit link is commonly used (i.e., logistic regression). Other link functions, such as the probit and complementary log-log, are also available (Dobson 1999). This example is described in more detail in the Section 12.2.

Non-Bayesian longitudinal and repeated measures models require a specification for the covariance or correlation structure of the data. Options include compound symmetric (a.k.a. exchangeable), autoregressive, Toeplitz, and others (Littell, Pendergast, and Natarajan 2000).

Tuning parameter optimization usually falls into one of two categories:
  - pre-define a set of parameter values to evaluate or
  - sequentially discover new parameter combinations based on previous results.

The use of pre-defined sets is commonly called grid search. The main choices involved in grid search are how to make the grid and how many parameter combinations to evaluate. Grid search is often judged as inefficient since the number of grid points required to cover the parameter space can grow unmanageable with the curse of dimensionality. There is truth to this concern, but it is most true when the process is not optimized. 

For sequential or iterative search methods, almost any nonlinear optimization method is appropriate, although some are more efficient than others. 

How can we signal to tidymodels functions which arguments should be optimized? Parameters are marked for tuning by assigning them a value of tune()
