---
title: "Machine Learning with `tidymodels`: Part 3" 
theme: readable
output:
  html_document:
    toc: true
    toc_float: true
    fig_width: 12
    fig_height: 7
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings({ 
  library(tidymodels)
  library(here)
  library(palmerpenguins)
  library(tidyverse)
  library(doParallel)
}))
# prefer tidymodels functions in any case of name conflict
tidymodels::tidymodels_prefer() 
```

## What we have covered so far 

Through the first two parts of this workshop we gave an overview of machine learning and the tools to fit machine learning workflows with `tidymodels`. We discussed how machine learning works best when we are interested in *prediction*. We also covered the basics of a machine learning workflow which were:

- How to appropriately spending our data into training and test sets
- Preprocess features when necessary to ensure the best model fit 
- build and fit models 
- Judge model effectiveness using appropriate metrics
- Resampling and cross validation to more effectively train and compare models
- How to tune our models appropriately to increase predictive accuracy

Each one of these steps is common no matter what language or structure you choose to implement your machine learning models. 

In this final part, we will explore some common supervised learning algorithms using `tidymodels`. The goal of this final part is to provide a high level overview of what common algorithms do.  

## Dataset 

For these exercises, we will use the Credit dataset from the `ISLR2` package.  

```{r}
credit <- ISLR2::Credit

glimpse(credit)

## Set up train and test set 
data_split <- initial_split(credit, prop = .75)

data_train <- training(data_split)
data_test <- testing(data_split)

## specify cross validation split with 10 folds 
data_kfolds <- vfold_cv(data_train, v = 10)
```


## Linear Regression

We have already worked with linear regression, so let's jump straight into setting up the details in `tidymodels`. 

```{r}
### Linear Regression Workflow 
lm_model <- linear_reg()%>%
  set_engine("lm")%>%
  set_model("regression")

## Note that we can put all of these in one argument
## But for readability it's reasonable to pipe. 
## You can choose in your own work 
alt_lm_model <- linear_reg(
  mode = "regression",
  engine = "lm"
)
```

Linear Regression is a supervised learning problem. We are looking for a function that predicts the value of our dependent variable based on a set of observed features from a sample of *n* units. Each algorithm has an associated *loss function*, and determines the optimal fit based on the function that has the lowest predicted loss. For linear regression, the loss function is the sum of the squared residuals. The function that minimizes this is the solution to a linear regression problem. 


## Regularization and the Lasso

Linear regressions are surprisingly good at prediction compared to alternative model specifications. None the less, alternative fitting procedures can yield better prediction accuracy and sometimes better model interpretability. 

If the true relationship between our dependent variable and features is approximately linear, linear regression (fit with ordinary least squares or OLS) will have low bias. In fact, given some assumptions about the nature of the error term an OLS fit is the *best linear unbiased estimator* possible. However, if our sample size is not much larger than the number of features linear regression can have a lot of variability in its fit, which leads to overfitting and poor predictions on the test set. Furthermore, if the reverse is true and the number of features is greater than the number of data points, linear regression does not have a solution at all. The variance is infinite! 

Regularizing or shrinking coefficients can substantially reduce the variance of estimates, which leads to an improvement in accuracy for out of sample prediction. There is no free lunch here though. Recall the bias-variance trade-off. Regularization and shrinking methods will increase the bias of an estimate, but such bias is often negligible in large samples. 

Let's cover two common methods for regularizing coefficients: *Ridge regression* and the *lasso*. 

Ridge regression is a method that solves the following minimization problem. 

$$\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda\sum_{j=1}^{p}\beta_j^2$$
In words, we want to find a coefficient estimate $\hat{\beta_j}$ that minimizes the total value of the residual sum of squares. Unlike linear regression, we have an additional term which is the shrinkage penalty. This term is small when the estimated coefficients are close to zero, and so *shrinks* the estimates of $\beta_j$ towards zero (James *et al.* 2021). 

The tuning parameter $\lambda \geq 0$ controls the relative impact of these two terms on the model estimates. In the extreme case when $\lambda = 0$, the model is identical to linear regression. In the other extreme case when $\lambda = \infty$ the regression coefficients will all be zero. Therefore, selecting the best $\lambda$ is critical for model fit and prediction. 

A note on fitting ridge regression models. Unlike linear regression, which is scale equivariant, ridge regression coefficient estimates will change substantially when multiplying a given feature by a constant. As an example, if we were interested in 1000s of dollars instead of dollars, multiplying our data point by $\frac{1}{1000}$ would not lead ridge regression to change the coefficient estimate by a factor of 1000. It may even change depending on the scaling of other predictor. For this reason, we should always apply ridge regression after standardizing our feature set. 

Here is how we set up ridge regression in `tidymodels` 

```{r}
ridge_model <- linear_reg(
  mixture = 0, # this specifies only ridge regression
  penalty = tune() # this specifies the penalty to apply. Tuning parameter
)%>%
  set_mode("regression")%>%
  # Using glmnet for fitting. Also necessary to set penalty
  # to make use of this engine
  set_engine("glmnet")

ridge_recipe <-recipe(formula = Rating ~ ., data = data_train)%>%
  step_dummy(all_nominal_predictors())%>%
  # Standardize our features 
  step_normalize(all_numeric_predictors())%>%
  step_zv(all_predictors())

ridge_wflow <- workflow()%>%
  add_recipe(ridge_recipe)%>%
  add_model(ridge_model)

## Tuning parameter set up. 
# Set up grid for lambda penalty 
lambda_grid <- grid_regular(
  penalty(range = c(-5, 5)),
  levels = 50
)
# Take a look at the output 
print(lambda_grid)

## Tune and fit models with tune_grid()
ridge_res <- tune_grid(
  ridge_wflow, 
  resamples = data_kfolds, 
  grid = lambda_grid
)

## Visualize the fit with autoplot()
autoplot(ridge_res)

## Select best metric. Since we didn't pass additional metrics we will use rmse which comes by default 
ridge_best <- select_best(ridge_res, metric = "rmse")

## Get our final model and finalize workflow
## We can now fit our model on the full training dataset 
ridge_spec <- ridge_wflow %>%
  finalize_workflow(parameters = ridge_best)%>%
  fit(data = data_test)

## We can now test it against our test set and see performance
augment(ridge_spec, new_data = data_test)%>%
  rmse(truth = Rating, estimate = .pred)
```

Ridge regression will include all of our features in the final model, though it may set many of the coefficients close to zero. This raises a question about model interpretation when the number of features is large. An alternative algorithm is the *lasso*. The goal of the lasso is to minimize the following equation. 

$$\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda\sum_{j=1}^{p}|\beta_j|$$
The lasso also shrinks the coefficient estimates towards zero, but it's penalty term forces some of the coefficient estimates to be exactly equal to zero if the tuning parameter is large. As a result, the lasso is a type of variable selection algorithm. A way to think about the lasso is that we are trying to find a set of coefficient estimates that give the smallest RSS, subject to a budget constraint for how large the second part of the equation can be. If the budget is large enough, the lasso will reproduce the linear regression estimate.

A weakness of the lasso compared to linear and ridge regression is that the lasso implicitly assumes that at least one of the coefficients is equal to zero if the penalty term is high enough. In situations where this is actually true, the lasso will outperform linear regression and ridge regression. In situations where it is not, the lasso will be sub-optimal. 

Here is how to fit the lasso in `tidymodels`

```{r}
## Lasso specification 
lasso_model <- linear_reg(
  penalty = tune(), 
  mixture = 1 
)%>%
  set_mode("regression")%>%
  set_engine("glmnet")

# Note that for this problem our recipes are the same as ridge regression. We could reuse the ridge recipe in our lasso workflow
lasso_recipe <- recipe(formula = Rating ~ ., data = data_train)%>%
  step_dummy(all_nominal_predictors())%>%
  step_normalize(all_predictors())

lasso_wflow <- workflow()%>%
  add_recipe(lasso_recipe)%>%
  add_model(lasso_model)

## We can reuse the same penalty grid and same cross-validation resamples. In practice, we would want to cast a wide initial net and then narrow down on a range. This is part of iteration steps in a machine learning analysis

## Tune model 
lasso_res <- tune_grid(
  lasso_wflow,
  resamples = data_kfolds,
  grid = lambda_grid
)

## select best penalty 
lasso_best <- lasso_res %>% 
  select_best(metric = "rmse")

## visualize fit 
autoplot(lasso_res)

## Select best value and refit to whole training set 
lasso_fit <- lasso_wflow %>% 
  finalize_workflow(lasso_best)%>%
  fit(data = data_test)

## See how it performs on our test data set 
augment(lasso_fit, new_data = data_test)%>%
  rmse(truth = Rating, estimate = .pred)
```

### Challenge 7 

Using the `challenge7` dataset, predict whether a penguin is a Gentoo using the remaining set of features with all three kinds of linear regression models we have seen so far. Which model performs the best on the test set? 


```{r}
# seed for this challenge 
set.seed(3811212514757)

challenge7 <- penguins %>% 
  mutate(isChinstrap = if_else(species == "Gentoo", TRUE, FALSE))

## Data spend 
## Set up train and test set 
c7_split <- initial_split(challenge7, prop = .75)

c7_train <- training(c7_split)
c7_test <- testing(c7_split)

## specify cross validation split with 10 folds 
c7_kfolds <- vfold_cv(c7_train, v = 10)
```

## Classification 

Machine learning breaks down the type of algorithm by whether we are interesting in predicting a quantitative or qualitative response variable. In this section, we will discuss classification. Predicting a qualitatative response is considered classification because we assign the observation to a category (or class). 

Classification is also a supervised learning technique because we have a set of labeled training data that we can use to build a classifier. 

A note before we move on to our next algorithm. Many machine learning textbooks suggest that linear regression is not an appropriate algorithm for classification. When interested in multi-class problems, this makes a lot of sense because linear regression will assume an ordering scheme that does not actually exist. The result will be poor out of sample fit. 

In the binary dependent variable case, we can use linear regression. Such a model is often referred to as a linear probability model. The results of this model are interpretable (each coefficient represents the change in probability of success holding all other constants equal) and will give similar classifications as other classification algorithms. 

### Logistic Regression 

Machine learning practitioners often recommend logistic regression as a starting model when we have a problem of predicting a binary outcome or probability. For example, if we are estimating the relationship between mortality and income we can estimate the probability of mortality given a change in income. 

We write this as $P[M|I]$ and the values will range between 0 and 1. We can make a prediction for any given value of income on the mortality outcome. Normally, we establish a threshold for prediction. For example, we might predict death where $P[M|i] > 0.5$. 

Logistic regression is a generalized linear model where we model the probability function using the logistic function. Define the general function as $p(Y= 1|X)$. Then the model of interest is: 

$$p(Y = 1|\textbf{X}) = \frac{e^{\textbf{X}\beta}}{1 + e^{\textbf{X}\beta}}$$
Here the bold X indicates a vector of features and the $\beta$ coefficient represents a vector of coefficients. 

R will fit this via maximum likelihood. For more discussion on the mathematical principles behind logistic regression, see p.135 of James *et al.*. 

Here's how to fit classification problems in `tidymodels` using logistic regression. 

For these classification problems, we will use the default dataset. 

```{r}
default <- ISLR2::Default

class_split <- initial_split(default, prop = 3/4)
class_train <- training(class_split)
class_test <- testing(class_split)
```

```{r}
## Set model specification 
class_spec <- logistic_reg()%>%
  set_mode("classification")%>%
  set_engine("glm")

## Make recipe 
class_recipe <- recipe(default ~ . ,data = class_train)%>%
  step_dummy(all_nominal_predictors())%>%
  step_normalize(all_predictors())

## Create workflow 
class_wflow <- workflow()%>%
  add_recipe(class_recipe)%>%
  add_model(class_spec)

## fit a model and see prediction metrics 
class_fit <- class_wflow %>% 
  fit(data = class_train)

augment(class_fit, new_data = class_test)%>%
  accuracy(truth = default, estimate = .pred_class)
```

### Naive Bayes 

Like linear regression, logistic regression is a rock solid initial model. However, when there is substantial differences between classes the parameter estimates for logistic regression become unstable. We may also have more than two classes, which becomes a bit more awkward to extent to the logistic case. 

Naive Bayes is an alternative algorithm that uses some results that follows Bayes' Theorem to provide accurate classification predictions. When the number of features is large or the number of training units is small, Naive Bayes's relative reduction in variance improves on predictions.

The classifier makes one assumption: within the $k^th$ class, the $p$ features are independent of each other. This assumption reduces the complexity of estimating the joint distribution of the features which is often difficult to estimate. While this is a modeling convenience and almost certainly false in general, having this assumption allows Naive Bayes to be surprisingly good at prediction. It will do less well than other algorithms in problems where interactions among features are important for classification. 
The structure of implementing Naive Bayes in `tidymodels` should look familiar. 

```{r}
naive_bayes <- naive_Bayes()%>%
  set_engline("klaR")%>%
  set_mode("classifcation")%>%
  # assume predictors come from normal distributions 
  set_args(usekernel = FALSE)

bayes_recipe <- recipe(default ~ ., data = class_train)

bayes_wflow <- workflow()%>%
  add_recipe(bayes_recipe)%>%
  add_model(naive_bayes)

bayes_fit <- bayes_wflow %>% 
  fit(data = data_train)
```


## Random Forests 

A decision tree is a method of stratifying the feature space into a number of simple regions. Usually, trees use the mean or mode dependent variable value for the training data in the region to which it belongs (James *et al.* 2021). Trees can be applied to both regression or classification problems. To build a single tree, we:

1. Divide the feature space into distinct regions. 
2. For every observation that falls into a distinct region ($R_j$) we make the same prediction. 

Trees by themselves turn out to not be very good at prediction compared to other supervised learning model. However, combine them into a *random forest* and they become very useful, especially if the relationship is nonlinear and complex. 

A random forest (Breiman 2001) is an example of an ensemble method, which is an approach that combines many simple models in order to get a single powerful prediction model. 

In a random forest, we build a number of decision trees on bootstrapped training samples. Each tree is grown independently on random samples of the observations, a process known as *bagging.*

When building the trees, each time a split occurs, we take a random sample of features to be split candidates instead of the full set of features. By choosing a random subset each time, a random forest decorrelates the individual decision trees. This makes the resulting average less variable. 

Here's how to fit tree-based methods in `tidymodels` 

```{r}
## model specification 
rf_model <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 500
  )%>%
  set_engine("ranger")%>%
  set_mode("classification")

rf_recipe <- ecipe(default ~ ., data = class_train)%>%
  step_normalize(all_predictors())

rf_wflow <- workflow()%>%
  add_recipe(rf_recipe)%>%
  add_model(rf_model)

### Use cross-validation to tune our model appropriately 

## First make a grid 
rf_grid <- control_grid()

## Now fine appropriate tuning parameters
rf_fit <- tune_grid(
  object = rf_wflow, 
  resamples = data_kfolds, 
  grid = 10,
  control = rf_grid
)

```

## Ensembles

A Random Forest is an example of an ensemble method, but we can also create our own models and put them together into an ensemble.

To put together an ensemble in `tidymodels` we need to: 

1. Define our candidate models. 
2. Assemble a stack. 
3. Determine how to combine the model predictions 
4. Fit the models 

We will take advantage of the models that we have already created. 

```{r}
ensemble_model <- stacks() %>% 
  ## Add candidate models 
  add_candidates(rf_fit)%>%
  add_candidates(bayes_fit)%>%
  add_candidate(class_fit)%>%
  ## Combine the predictions 
  blend_predictions()%>%
  ## Fit candidates with nonzero stacking coefficients 
  fit_members()

## See our model 
ensemble_model

## See which model configurations were used 
ensemble_model %>% 
  collect_parameters()
```

### Challenge 8: Putting everything together 

Using the same dataset as Challenge 7, fit at least three different models working through the process we have covered in this workshop. The goal for this problem is to correctly classify penguins into "Gentoo" or "Not Gentoo" types. 

```{r}
set.seed(1234)

## Read in data 

## Data spend 

## Create Model specifications 

## Create Recipes 

## Fit and tune model specifications 

## Choose the best one 

## Evaluate the model

```