---
title: "Machine Learning with `tidymodels`: Part 3" 
theme: readable
output:
  html_document:
    toc: true
    toc_float: true
    fig_width: 12
    fig_height: 7
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(suppressWarnings({ 
  library(tidymodels)
  library(here)
  library(palmerpenguins)
  library(tidyverse)
  library(doParallel)
}))
# prefer tidymodels functions in any case of name conflict
tidymodels::tidymodels_prefer() 
```

## What we have covered so far 

Through the first two parts of this workshop we gave an overview of machine learning and the tools to fit machine learning workflows with `tidymodels`. We discussed how machine learning works best when we are interested in *prediction*. We also covered the basics of a machine learning workflow which were:

- How to appropriately spending our data into training and test sets
- Preprocess features when necessary to ensure the best model fit 
- build and fit models 
- Judge model effectiveness using appropriate metrics
- Resampling and cross validation to more effectively train and compare models
- How to tune our models appropriately to increase predictive accuracy

Each one of these steps is common no matter what language or structure you choose to implement your machine learning models. 

In this final part, we will explore some common supervised learning algorithms using `tidymodels`. The goal of this final part is to provide a high level overview of what common algorithms do.  

## Linear Regression

We have already worked with linear regression, so let's jump straight into setting up the details in `tidymodels`. 

```{r}
### Linear Regression Workflow 
lm_model <- linear_reg()%>%
  set_engine("lm")%>%
  set_model("regression")

## Note that we can put all of these in one argument
## But for readability it's reasonable to pipe. 
## You can choose in your own work 
alt_lm_model <- linear_reg(
  mode = "regression",
  engine = "lm"
)
```

Linear Regression is a supervised learning problem. We are looking for a function that predicts the value of our dependent variable based on a set of observed features from a sample of *n* units. Each algorithm has an associated *loss function*, and determines the optimal fit based on the function that has the lowest predicted loss. For linear regression, the loss function is the sum of the squared residuals. The function that minimizes this is the solution to a linear regression problem. 


## Regularization and the Lasso

Linear regressions are surprisingly good at prediction compared to alternative model specifications. None the less, alternative fitting procedures can yield better prediction accuracy and sometimes better model interpretability. 

If the true relationship between our dependent variable and features is approximately linear, linear regression (fit with ordinary least squares or OLS) will have low bias. In fact, given some assumptions about the nature of the error term an OLS fit is the *best linear unbiased estimator* possible. However, if our sample size is not much larger than the number of features linear regression can have a lot of variability in its fit, which leads to overfitting and poor predictions on the test set. Furthermore, if the reverse is true and the number of features is greater than the number of data points, linear regression does not have a solution at all. The variance is infinite! 

Regularizing or shrinking coefficients can substantially reduce the variance of estimates, which leads to an improvement in accuracy for out of sample prediction. There is no free lunch here though. Recall the bias-variance trade-off. Regularization and shrinking methods will increase the bias of an estimate, but such bias is often negligible in large samples. 

Let's cover two common methods for regularizing coefficients: *Ridge regression* and the *lasso*. 

Ridge regression is a method that solves the following minimization problem. 

$$\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda\sum_{j=1}^{p}\beta_j^2$$
In words, we want to find a coefficient estimate $\hat{\beta_j}$ that minimizes the total value of the residual sum of squares. Unlike linear regression, we have an additional term which is the shrinkage penalty. This term is small when the estimated coefficients are close to zero, and so *shrinks* the estimates of $\beta_j$ towards zero (James *et al.* 2021). 

The tuning parameter $\lambda \geq 0$ controls the relative impact of these two terms on the model estimates. In the extreme case when $\lambda = 0$, the model is identical to linear regression. In the other extreme case when $\lambda = \infty$ the regression coefficients will all be zero. Therefore, selecting the best $\lambda$ is critical for model fit and prediction. 

A note on fitting ridge regression models. Unlike linear regression, which is scale equivariant, ridge regression coefficient estimates will change substantially when multiplying a given feature by a constant. As an example, if we were interested in 1000s of dollars instead of dollars, multiplying our data point by $\frac{1}{1000}$ would not lead ridge regression to change the coefficient estimate by a factor of 1000. It may even change depending on the scaling of other predictor. For this reason, we should always apply ridge regression after standardizing our feature set. 

Here is how we set up ridge regression in `tidymodels` 

```{r}
ridge_model <- linear_reg(
  mixture = 0, # this specifies only ridge regression
  penalty = tune() # this specifies the penalty to apply. Tuning parameter
)%>%
  set_mode("regression")%>%
  # Using glmnet for fitting. Also necessary to set penalty
  # to make use of this engine
  set_engine("glmnet")

ridge_recipe <-recipe(formula = Y ~ ., data = data)%>%
  step_dummy(all_nominal_predictors())%>%
  # Standardize our features 
  step_normalize(all_predictors())

ridge_wflow <- workflow()%>%
  add_recipe(ridge_recipe)%>%
  add_model(ridge_spec)

## Set up train and test set 
data_split <- initial_split(data, prop = .75)

data_train <- training(data_split)
data_test <- testing(data_split)

## specify cross validation split with 10 folds 
data_kfolds <- vfold_cv(data_train, v = 10)

## Tuning parameter set up. 
# Set up grid for lambda penalty 
lambda_grid <- grid_regular(
  penalty(range = c(-5, 5)),
  levels = 50
)
# Take a look at the output 
print(lambda_grid)

## Tune and fit models with tune_grid()
ridge_res <- tune_grid(
  ridge_wflow, 
  resamples = data_kfolds, 
  grid = lambda_grid
)

## Visualize the fit with autoplot()
autoplot(ridge_res)

## Select best metric. Since we didn't pass additional metrics we will use rmse which comes by default 
ridge_best <- select_best(ridge_res, metric = "rmse")

## Get our final model and finalize workflow
## We can now fit our model on the full training dataset 
ridge_spec <- ridge_wflow %>%
  finalize_workflow(parameters = ridge_best)%>%
  fit(data = data_train)

## We can now test it against our test set and see performance
augment(ridge_spec, new_data = data_test)%>%
  rmse(truth = Y, estimate = .pred)
```

Ridge regression will include all of our features in the final model, though it may set many of the coefficients close to zero. This raises a question about model interpretation when the number of features is large. An alternative algorithm is the *lasso*. The goal of the lasso is to minimize the following equation. 

$$\sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda\sum_{j=1}^{p}|\beta_j|$$
The lasso also shrinks the coefficient estimates towards zero, but it's penalty term forces some of the coefficient estimates to be exactly equal to zero if the tuning parameter is large. As a result, the lasso is a type of variable selection algorithm. A way to think about the lasso is that we are trying to find a set of coefficient estimates that give the smallest RSS, subject to a budget constraint for how large the second part of the equation can be. If the budget is large enough, the lasso will reproduce the linear regression estimate.

A weakness of the lasso compared to linear and ridge regression is that the lasso implicitly assumes that at least one of the coefficients is equal to zero if the penalty term is high enough. In situations where this is actually true, the lasso will outperform linear regression and ridge regression. In situations where it is not, the lasso will be sub-optimal. 

Here is how to fit the lasso in `tidymodels`

```{r}
## Lasso specification 
lasso_model <- linear_reg(
  penalty = tune(), 
  mixture = 1 
)%>%
  set_mode("regression")%>%
  set_engine("glmnet")

# Note that for this problem our recipes are the same as ridge regression. We could reuse the ridge recipe in our lasso workflow
lasso_recipe <- recipe(formula = Y ~ ., data = data_train)%>%
  step_dummy(all_nominal_predictors())%>%
  step_normalize(all_predictors())

lasso_wflow <- workflow()%>%
  add_recipe(lasso_recipe)%>%
  add_model(lasso_model)

## We can reuse the same penalty grid and same cross-validation resamples. In practice, we would want to cast a wide initial net and then narrow down on a range. This is part of iteration steps in a machine learning analysis

## Tune model 
lasso_res <- tune_grid(
  lasso_wflow,
  resamples = data_kfolds,
  grid = penalty_grid
)

## select best penalty 
lasso_best <- lasso_res %>% 
  select_best(metric = "rmse")

## visualize fit 
autoplot(lasso_res)

## Select best value and refit to whole training set 
lasso_fit <- lasso_wflow %>% 
  finalize_workflow(lasso_best)%>%
  fit(data = data_train)

## See how it performs on our test data set 
augment(lasso_fit, new_data = data_test)%>%
  rmse(truth = Y, estimate = .pred)
```

## Classification 

## Random Forests 

## Support Vector Machines

## Ensembles