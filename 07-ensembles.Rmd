# Ensembles

## Load packages

```{r message=FALSE, warning=FALSE}

library(ck37r)
library(SuperLearner)
library(vip)
library(tidymodels) # tidymodels framework 
library(here) # reproducible way to find files

theme_set(theme_minimal())

```

## Load data

Load `train_x_class`, `train_y_class`, `test_x_class`, and `test_y_class` variables we defined in 02-preprocessing.Rmd for this *classification* task.

```{r}
# Objects: task_reg, task_class
load(here("data", "preprocessed.RData"))
```

## Overview

### Stacking/Super Learning 

Wolpert, D.H., 1992. [Stacked generalization](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.1533). *Neural networks*, 5(2), pp.241-259.

Breiman, L., 1996. [Stacked regressions]((https://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf). *Machine learning*, 24(1), pp.49-64.

### SuperLearner 

The ["SuperLearner" R package](https://cran.r-project.org/web/packages/SuperLearner/index.html) is a method that simplifies ensemble learning by allowing you to simultaneously evaluate the cross-validated performance of multiple algorithms and/or a single algorithm with differently tuned hyperparameters. This is a generally advisable approach to machine learning instead of fitting single algorithms.

Let's see how the four classification algorithms you learned in this workshop (1-lasso, 2-decision tree, 3-random forest, and 4-gradient boosted trees) compare to each other and also to 5-binary logistic regression (`glm`) and to the 6-mean of Y as a benchmark algorithm, in terms of their cross-validated error!

A "wrapper" is a short function that adapts an algorithm for the SuperLearner package. Check out the different algorithm wrappers offered by SuperLearner:

### Choose algorithms

```{r}
# Review available models 
SuperLearner::listWrappers()
```

```{r cvsl_fit, cache = TRUE}
# Compile the algorithm wrappers to be used.
sl_lib <- c("SL.mean", # Marginal mean of the outcome () 
            "SL.glmnet", # GLM with lasso/elasticnet regularization 
            "SL.rpart", # Decision tree 
            "SL.ranger", # Random forest  
            "SL.xgboost") # Xgbboost 

```

## Non-tidy

### Fit model

Fit the ensemble!

```{r}
# This is a seed that is compatible with multicore parallel processing.
# See ?set.seed for more information.
set.seed(1, "L'Ecuyer-CMRG") 

# This will take a few minutes to execute - take a look at the .html file to see the output!
cv_sl <-  SuperLearner::CV.SuperLearner(
  Y = as.numeric(as.character(train_y_class)),
  X = train_x_class,
  family = binomial(),
  # For a real analysis we would use V = 10.
  cvControl = list(V = 5L, stratifyCV = TRUE),
  SL.library = sl_lib,
  verbose = FALSE)
```

### Risk

Risk is a performance estimate - it's the average loss, and loss is how far off the prediction was for an individual observation. The lower the risk, the fewer errors the model makes in its prediction. SuperLearner's default loss metric is squared error $(y_{actual} - y_{predicted})^2$, so the risk is the mean-squared error (just like in ordinary least *squares* regression). View the summary, plot results, and compute the Area Under the ROC Curve (AUC)!

#### Summary 

* `Discrete SL` chooses the best single learner (in this case, `SL.glmnet` or `lasso`).
* `SuperLearner` takes a weighted average of the **models** using the coefficients (importance of each individual learner in the overall ensemble). Coefficient 0 means that learner is not used at all.
* `SL.mean_All` (the weighted mean of $Y$) is a benchmark algorithm (ignoring features). 

```{r}

summary(cv_sl)

```

#### Plot

```{r cvsl_review}
# Plot the cross-validated risk estimate with 95% CIs.

plot(cv_sl)

```

### Compute AUC for all estimators

**ROC**

ROC: an ROC (receiver operating characteristic curve) plots the relationship between True Positive Rate (Y-axis) and FALSE Positive Rate (X-axis). 

![Area Under the ROC Curve](https://developers.google.com/machine-learning/crash-course/images/AUC.svg)

**AUC** 

AUC: Area Under the ROC Curve 

1 = perfect 

0.5 = no better than chance 

```{r}
auc_table(cv_sl)
```

### Plot the ROC curve for the best estimator (DiscretSL)

```{r}
plot_roc(cv_sl)
```

### Review weight distribution for the SuperLearner

```{r}
print(cvsl_weights(cv_sl), row.names = FALSE)
```

General stacking approach is available in the tidymodels framework through [`stacks`](https://github.com/tidymodels/stacks) package (developmental stage). 

However, SuperLearner is currently not available in the tidymodels framework. If you'd like to, you can easily build and add a parsnip model. Here, I just show a snapshot of the whole process. If you are interested in knowing more about it, please take a look at [this vignette](https://www.tidymodels.org/learn/develop/models/) of the tidymodels.

```{r}
# Set model 
set_new_model("superlearner")

# Set mode 
set_model_mode(model = "superlearner", 
               mode = "classification")

# Set model engine 
set_model_engine(
  "superlearner",
  mode = "classification",
  eng = "SuperLearner"
)

# Set dependency 
set_dependency("superlearner", 
               eng = "SuperLearner", pkg = "SuperLearner")

# Show model info 
show_model_info("superlearner")

# Add arguments 
set_model_arg(
  model = "superlearner",
  eng = "SuperLearner",
  parsnip = "cv_control",
  original = "cvControl",
  func = list(pkg = "SuperLearner", 
              fun = "CV.SuperLearner"),
  has_submodel = TRUE # Are you making multiple iterations?
)

show_model_info("superlearner")
```

## Challenge 5

Open Challenge 5 in the "Challenges" folder. This challenge provides some practice with SuperLearner. Note that this is a harder challenge than others.  

A longer tutorial on SuperLearner is available here: (https://github.com/ck37/superlearner-guide)